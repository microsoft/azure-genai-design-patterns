{"Question": "What is the Incorrect behavior rate for gpt-3.5-turbo on Sensitive Prompts and text-davinci-003 on Disallowed Prompts?", "Answer": "According the graph in Figure 6: Example Prompt for RBHM gpt-3.5-turbo's Incorrect behavir rate for Sensitive Prompts is approximately 41%. There was a 22% Incorrect behavior rate for text-davinci-003 on Disallowed Prompts."}
{"Question": "What are the steps used to generate synthetic hallucination data using GPT-4?", "Answer": "1. Pass a prompt through GPT-4 model and get a response\n2. Pass prompt + response through GPT-4 with an instruction to list all hallucinations\n  (a) If no hallucinations are found, continue\n3. Pass prompt + response + hallucinations through GPT-4 with an instruction to rewrite the response without hallucinations\n4. Pass prompt + new response through GPT-4 with an instruction to list all hallucinations\n  (a) If none are found, keep (original response, new response) comparison pair\n  (b) Otherwise, repeat up to 5x"}  
{"Question": "What were gpt-4's scores against the TruthfulQA mc1 data set?", "Answer": "The gpt-4-base model had an accuracy score of approximately 30% with 0-shot prompt engineering, increasing to around 40% with 5-shot prompting. It tops out at appoximately 50% with RLHF fine-tuning."}
{"question": "What are the steps OpenAI is taking to make models safer and more aligned?", "answer": "We will continue to learn from deployment and will update our models to make them safer and more aligned. This will include incorporating lessons from real-world data and usage, including instances of adversarial system messages that we detect early in the process of ramping up model access. Additionally, there are a few key steps that we are taking and encourage other developers of language models to adopt:  Adopt layers of mitigations throughout the model system: As models get more powerful and are adopted more widely, it is critical to have multiple levels of defense, including changes to the model itself, oversight and monitoring of model usage, and product design for safe usage.  Build evaluations, mitigations, and approach deployment with real-world usage in mind: Context of use such as who the users are, what the specific use case is, where the model is being deployed, etc., is critical to mitigating actual harms associated with language models and ensuring their deployment is as beneficial as possible. It\u2019s particularly important to account for real-world vulnerabilities, humans roles in the deployment context, and adversarial attempts. We especially encourage the development of high quality evaluations and testing of model mitigations on datasets in multiple languages.  Ensure that safety assessments cover emergent risks: As models get more capable, we should be prepared for emergent capabilities and complex interactions to pose novel safety issues. It\u2019s important to develop evaluation methods that can be targeted at advanced capabilities that could be particularly dangerous if they emerged in future models, while also being open-ended enough to detect unforeseen risks.  Be cognizant of, and plan for, capability jumps “in the wild”: Methods like fine-tuning and chain-of-thought prompting could lead to capability jumps in the same base model. This should be accounted for explicitly in internal safety testing procedures and evaluations. And a precautionary principle should be applied: above a safety critical threshold, assurance of sufficient safety is required."} 
{"question": "What steps did OpenAI take to better understand acceleration risk from the deployment of GPT-4?", "answer": "In order to specifically better understand acceleration risk from the deployment of GPT-4, we recruited expert forecasters to predict how tweaking various features of the GPT-4 deployment (e.g., timing, communication strategy, and method of commercialization) might affect (concrete indicators of) acceleration risk. Forecasters predicted several things would reduce acceleration, including delaying deployment of GPT-4 by a further six months and taking a quieter communications strategy around the GPT-4 deployment (as compared to the GPT-3 deployment). We also learned from recent deployments that the effectiveness of quiet communications strategy in mitigating acceleration risk can be limited, in particular when novel accessible capabilities are concerned."} 
{"question": "What should be a crucial consideration for policymakers and other stakeholders regarding the impact of GPT-4 on the economy and workforce?", "answer": "The impact of GPT-4 on the economy and workforce should be a crucial consideration for policymakers and other stakeholders. While existing research primarily focuses on how AI and generative models can augment human workers, GPT-4 or subsequent models may lead to the automation of certain jobs. This could result in workforce displacement. Over time, we expect GPT-4 to impact even jobs that have historically required years of experience and education, such as legal services."} 
{"question": "What types of harmful content can GPT-4-early generate?", "answer": "As an example, GPT-4-early can generate instances of hate speech, discriminatory language, incitements to violence, or content that is then used to either spread false narratives or to exploit an individual. Such content can harm marginalized communities, contribute to hostile online environments, and, in extreme cases, precipitate real-world violence and discrimination. In particular, we found that intentional probing of GPT-4-early could lead to the following kinds of harmful content: 1. Advice or encouragement for self harm behaviors 2. Graphic material such as erotic or violent content 3. Harassing, demeaning, and hateful content 4. Content useful for planning attacks or violence 5. Instructions for finding illegal content."}